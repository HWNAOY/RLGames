{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Minesweeper_Reinforce_with_baseline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO2nyXY2OuasMj8zBhLYdRf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arjunmenonv/RLGames/blob/master/Minesweeper/Minesweeper_Reinforce_with_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZDrkmmwokVR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "93a27583-46ee-4b01-c388-447d20137953"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvsxBbIpS22w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c935e5d3-5103-45a2-8d2b-b9c27f40bcd5"
      },
      "source": [
        "!cp \"\" "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: missing destination file operand after 'gdrive/My Drive/Colab Notebooks/RL_Games_Minesweeper/minesweeper_pygame.py'\n",
            "Try 'cp --help' for more information.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAEWFaF0o01-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "cf449638-8636-408d-bd01-515c440838aa"
      },
      "source": [
        "!pip install pygame\n",
        "!pip install mss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pygame in /usr/local/lib/python3.6/dist-packages (1.9.6)\n",
            "Requirement already satisfied: mss in /usr/local/lib/python3.6/dist-packages (5.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUVKPG48o3FM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import gym\n",
        "import collections\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras import optimizers\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.initializers import glorot_normal\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qS2EwMpmpNzP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from minesweeper_pygame import Minesweeper\n",
        "env = Minesweeper(display=False, ROWS = 6, COLS = 6, MINES = 6,OUT='CONDENSED', rewards = {\"win\" : 1, \"loss\" : -1, \"progress\" : 0.9, \"noprogress\" : -0.3, \"YOLO\" : -0.3})\n",
        "min_state = env.stateConverter(env.get_state()).flatten()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_i3u_6GepQcB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RANDOM_SEED=1\n",
        "np.random.seed(RANDOM_SEED)\n",
        "tf.random.set_seed(RANDOM_SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llDyx3QypTwk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_winrate(env,actor):\n",
        "    games = 0\n",
        "    moves = 0\n",
        "    stuck = 0\n",
        "    won_games = 0\n",
        "    lost_games = 0\n",
        "    r = 0\n",
        "    while games < 10000:\n",
        "        while True:\n",
        "            state = env.stateConverter(env.get_state())\n",
        "\n",
        "            action_probs = actor.predict(np.array([state]))\n",
        "            action_probs=action_probs.ravel()\n",
        "\n",
        "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            moves += 1\n",
        "\n",
        "            if reward == 1:\n",
        "                won_games += 1\n",
        "            if reward == -1:\n",
        "                lost_games += 1\n",
        "\n",
        "            if done:\n",
        "                games += 1\n",
        "                env.reset()\n",
        "                moves = 0\n",
        "                break\n",
        "            elif moves >= 30:\n",
        "                stuck += 1\n",
        "                games += 1\n",
        "                \n",
        "                lost_games +=1\n",
        "                env.reset()\n",
        "                moves = 0\n",
        "                break\n",
        "    return(won_games/games)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hj0xbA2Ope1K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Note: The advantage here is the total rewards. Hence, y_true during train_on_batch has magnitude of the total rewards!\n",
        "# I have added an extra 1 with y_pred so that the loss does not go to infinity and to avoid NaN error\n",
        "def custom_loss(y_true,y_pred):\n",
        "        loss =  -tf.reduce_mean(y_true*tf.math.log(y_pred+1))\n",
        "        return loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dad4rYEcphNI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def PolicyEstimator(env,lr=0.0001):\n",
        "  # Neural network\n",
        "  n_inputs=env.stateConverter(env.get_state()).shape\n",
        "  n_hidden = 288\n",
        "  n_hidden2 = 220\n",
        "  n_hidden3 = 200\n",
        "  n_outputs = env.ROWS*env.COLS\n",
        "\n",
        "  with strategy.scope():\n",
        "\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(18, kernel_size=(5, 5), strides=(1, 1),kernel_initializer=glorot_normal(), activation='relu',padding='same', input_shape=n_inputs))\n",
        "    model.add(Conv2D(36, kernel_size=(3, 3), strides=(1, 1),kernel_initializer=glorot_normal(),activation='relu', padding='same'))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(n_hidden, activation='relu'))\n",
        "    model.add(Dense(n_hidden2, activation='relu'))\n",
        "    model.add(Dense(n_hidden3, activation='relu'))\n",
        "\n",
        "    model.add(Dense(n_outputs, activation='softmax'))\n",
        "\n",
        "    model.compile(loss=custom_loss, optimizer=Adam(learning_rate=lr))\n",
        "\n",
        "  print(model.summary())\n",
        "\n",
        "  return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pI5e8YaPqWkx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ValueEstimator(env,lr=0.0001):\n",
        "  # Neural network\n",
        "  n_inputs=env.stateConverter(env.get_state()).shape\n",
        "  n_hidden = 288\n",
        "  n_hidden2 = 220\n",
        "  n_hidden3 = 200\n",
        "\n",
        "\n",
        "  with strategy.scope():\n",
        "\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(18, kernel_size=(5, 5), strides=(1, 1),kernel_initializer=glorot_normal(), activation='relu',padding='same', input_shape=n_inputs))\n",
        "    model.add(Conv2D(36, kernel_size=(3, 3), strides=(1, 1),kernel_initializer=glorot_normal(),activation='relu', padding='same'))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(n_hidden, activation='relu'))\n",
        "    model.add(Dense(n_hidden2, activation='relu'))\n",
        "    model.add(Dense(n_hidden3, activation='relu'))\n",
        "\n",
        "    model.add(Dense(1, activation=None))\n",
        "\n",
        "    model.compile(loss='mse', optimizer=Adam(learning_rate=lr))\n",
        "\n",
        "  print(model.summary())\n",
        "\n",
        "  return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCq7ayw-pjUM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reinforce_with_baseline(env,lr=0.0001,num_episodes=5000,discount_factor=0.99):\n",
        "\n",
        "    actor = PolicyEstimator(env,lr)\n",
        "    baseline=ValueEstimator(env,lr)\n",
        "\n",
        "    wins=0\n",
        "\n",
        "    \n",
        "    Transition = collections.namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "\n",
        "    episode_lengths=np.zeros(num_episodes)\n",
        "    episode_rewards=np.zeros(num_episodes)\n",
        "\n",
        "\n",
        "\n",
        "    for game in range(num_episodes):\n",
        "\n",
        "        done = False\n",
        "        env.reset()\n",
        "        state= env.stateConverter(env.get_state())\n",
        "\n",
        "        episode = []\n",
        "        t_step=0\n",
        "\n",
        "        states, actions, rewards = [], [], []\n",
        "        while not done and t_step<6*6:\n",
        "            t_step += 1\n",
        "\n",
        "\n",
        "            action_probs = actor.predict(np.array([state]))\n",
        "            action_probs=action_probs.ravel()\n",
        "\n",
        "            # print(action_probs)\n",
        "\n",
        "\n",
        "            if np.isnan(action_probs).any():\n",
        "              break\n",
        "\n",
        "\n",
        "            # for i in range(len(action_probs)):\n",
        "            #   if i not in env.action_space():\n",
        "            #     action_probs[i]=0\n",
        "              \n",
        "            # action_probs/=sum(action_probs)\n",
        "\n",
        "            \n",
        "\n",
        "\n",
        "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            if reward==1:\n",
        "              wins+=1\n",
        "\n",
        "\n",
        "            # next_state=next_state.flatten()\n",
        "            episode.append(Transition(\n",
        "              state=state, action=action, reward=reward, next_state=next_state, done=done))\n",
        "            state = next_state\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        flag=0\n",
        "        for t, transition in enumerate(episode):\n",
        "\n",
        "            # if not done:\n",
        "            #   break\n",
        "\n",
        "\n",
        "            # The return after this timestep\n",
        "            total_return = sum(discount_factor**i * trans.reward for i, trans in enumerate(episode[t:]))\n",
        "            if flag==0:\n",
        "\n",
        "              recorded_return=total_return\n",
        "              flag=1\n",
        "\n",
        "            # Calculate baseline/advantage\n",
        "            baseline_value = baseline.predict(np.array([transition.state]))\n",
        "\n",
        "\n",
        "            advantage = total_return- baseline_value\n",
        "\n",
        "            one_hot_output = np.zeros(36)\n",
        "            one_hot_output[transition.action] = advantage + 1e-5\n",
        "\n",
        "            baseline.train_on_batch(np.array([transition.state]),np.array([total_return]))\n",
        "\n",
        "            actor.train_on_batch( np.array([transition.state]), np.expand_dims(one_hot_output,axis=0))\n",
        "\n",
        "            \n",
        "\n",
        "\n",
        "        print('episode:',(game+1),'score:',(recorded_return),'timesteps:',(t_step))\n",
        "\n",
        "        episode_rewards[game] += recorded_return\n",
        "        episode_lengths[game] = t_step\n",
        "\n",
        "\n",
        "    wins=get_winrate(env,actor)\n",
        "    print('win_rate=')\n",
        "    print(wins*100)\n",
        "\n",
        "    actor.save('/content/gdrive/My Drive/Colab Notebooks/RL_Games_Minesweeper/Models/reinforce_with_baseline_model_v2.h5')\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.subplot(211)\n",
        "    plt.plot(episode_rewards)\n",
        "    plt.xlabel('episodes'); plt.ylabel('rewards')\n",
        "    plt.xlim((0, len(episode_rewards)))\n",
        "    plt.legend(loc=1); plt.grid()\n",
        "    plt.subplot(212)\n",
        "    plt.plot(episode_lengths)\n",
        "    plt.xlabel('episodes'); plt.ylabel('time_steps')\n",
        "    plt.xlim((0, len(episode_lengths)))\n",
        "    plt.legend(loc=4); plt.grid()\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "    return actor\n",
        "         \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uLLkdlqqHpF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%time\n",
        "agent = reinforce_with_baseline(env,num_episodes=200000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0Or5y_z2b8f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "721eef15-ed88-40eb-f1c6-9fa98a21a4ef"
      },
      "source": [
        "print('hello')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hello\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJzNflzKv58U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}